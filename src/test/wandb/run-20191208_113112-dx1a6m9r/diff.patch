diff --git a/src/nn/MLP.py b/src/nn/MLP.py
index f3a5058..ef4a5ab 100644
--- a/src/nn/MLP.py
+++ b/src/nn/MLP.py
@@ -12,7 +12,7 @@ class MLPConfig(ConfigBase):
             'input_dimension': 32,
             'output_dimension': 32,
             'activation': 'mish',
-            'out_activation': 'mish',
+            'out_activation': None,
             'num_neurons': [64, 64],
             'normalization': None,
             'weight_init': 'xavier',
diff --git a/src/rl/QmixNetwork.py b/src/rl/QmixNetwork.py
index 819f260..9c18e55 100644
--- a/src/rl/QmixNetwork.py
+++ b/src/rl/QmixNetwork.py
@@ -32,15 +32,15 @@ class QmixNetwork(torch.nn.Module):
 
         self.submixer_conf = conf.submixer
         self.supmixer_gc_conf = conf.supmixer_gc
-        self.supmixer_gnn_conf = conf.supmixer_gnn
+        #self.supmixer_gnn_conf = conf.supmixer_gnn
         self.supmixer_mlp_conf = conf.supmixer_mlp
 
         self.submixer = Qmixer(self.submixer_conf)
 
         # choose among two options on supmixer
 
-        # self.supmixer = GCN(**self.supmixer_gc_conf) # opt 1: GCN supmixer
-        self.supmixer = MLP(**self.supmixer_mlp_conf) # opt 2: MLP supmixer
+        self.supmixer = GCN(**self.supmixer_gc_conf)  # opt 1: GCN supmixer
+        # self.supmixer = MLP(**self.supmixer_mlp_conf) # opt 2: MLP supmixer
         self.supmixer_b = MLP(**self.supmixer_mlp_conf)
 
         self.n_clusters = self.submixer_conf.mixer['num_clusters']
@@ -52,31 +52,31 @@ class QmixNetwork(torch.nn.Module):
         aggregated_q = sub_q_ret_dict['qs']  # [#. graph x #.cluster]
         ws = sub_q_ret_dict['ws']  # [#. allies x #. clusters]
 
-        #### GCN: slow implementation ####
-        # graphs = dgl.unbatch(graph)
-        # nums_ally = get_number_of_ally_nodes(graphs)
-        #
-        # adj_mats = []
-        # for w in ws.split(nums_ally):
-        #     adj_mat = w.t().mm(w) + torch.eye(self.n_clusters, device=ws.device)  # [#. clusters x #. clusters]
-        #     adj_mats.append(adj_mat)
-        # adj_mats = torch.stack(adj_mats)  # [#. graph x #. clusters x #. clusters]
-        # sup_ws = self.supmixer(input=aggregated_feat, adj=adj_mats)  # [#. graph x #. clusters x 1]
-        #### slow implementation ####
-
-        #### MLP Style ####
+        ### GCN: slow implementation ####
         graphs = dgl.unbatch(graph)
         nums_ally = get_number_of_ally_nodes(graphs)
 
-        wss = []
+        adj_mats = []
         for w in ws.split(nums_ally):
-            w_agg = w.sum(0)
-            wss.append(w_agg)
-        wss = torch.stack(wss)  # [# graph X # clusters]
-        aggregated_feat = aggregated_feat * wss.unsqueeze(dim=-1)  # [# graph X # clusters X feature dim]
+            adj_mat = w.t().mm(w) + torch.eye(self.n_clusters, device=ws.device)  # [#. clusters x #. clusters]
+            adj_mats.append(adj_mat)
+        adj_mats = torch.stack(adj_mats)  # [#. graph x #. clusters x #. clusters]
+        sup_ws = self.supmixer(input=aggregated_feat, adj=adj_mats)  # [#. graph x #. clusters x 1]
+        ### slow implementation ####
 
-        sup_ws = self.supmixer(aggregated_feat)
-        #### MLP Style ####
+        # #### MLP Style ####
+        # graphs = dgl.unbatch(graph)
+        # nums_ally = get_number_of_ally_nodes(graphs)
+        #
+        # wss = []
+        # for w in ws.split(nums_ally):
+        #     w_agg = w.sum(0)
+        #     wss.append(w_agg)
+        # wss = torch.stack(wss)  # [# graph X # clusters]
+        # aggregated_feat = aggregated_feat * wss.unsqueeze(dim=-1)  # [# graph X # clusters X feature dim]
+        #
+        # sup_ws = self.supmixer(aggregated_feat)
+        # #### MLP Style ####
 
         sup_ws = torch.nn.functional.softmax(sup_ws, dim=1)
 
diff --git a/src/rl/Qnet.py b/src/rl/Qnet.py
index 0dad9c3..bea57f8 100644
--- a/src/rl/Qnet.py
+++ b/src/rl/Qnet.py
@@ -9,6 +9,8 @@ from src.config.graph_config import NODE_ALLY, EDGE_ENEMY
 from src.nn.MLP import MLPConfig
 from src.config.nn_config import VERY_LARGE_NUMBER
 
+from src.util.train_util import dn
+
 
 class QnetConfig(ConfigBase):
 
@@ -23,8 +25,11 @@ class QnetConfig(ConfigBase):
                      'exploration_method': 'eps_greedy'}
 
         self.move_module = mlp_conf
+        self.move_module['normalization'] = 'layer'
         self.move_module['out_activation'] = None
+
         self.attack_module = mlp_conf
+        self.attack_module['normalization'] = 'layer'
         self.attack_module['out_activation'] = None
 
 
@@ -54,6 +59,7 @@ class Qnet(nn.Module):
         move_arg, attack_arg = self(graph, node_feature, maximum_num_enemy)
 
         qs = torch.cat((move_arg, attack_arg), dim=-1)  # of all units including enemies
+        np_qs = dn(qs)
 
         ally_node_type_index = self.conf.qnet['ally_node_type_index']
         ally_node_indices = get_filtered_node_index_by_type(graph, ally_node_type_index)
