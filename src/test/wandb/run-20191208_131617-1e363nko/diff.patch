diff --git a/src/brain/QmixBrain.py b/src/brain/QmixBrain.py
index 5401f7c..cb2096e 100644
--- a/src/brain/QmixBrain.py
+++ b/src/brain/QmixBrain.py
@@ -17,7 +17,7 @@ class QmixBrainConfig(ConfigBase):
             'eps': 0.5,
             'eps_gamma': 0.995,
             'eps_min': 0.01,
-            'use_double_q': True,
+            'use_double_q': False,
             'use_clipped_q': True,
             'mixer_use_hidden': True,
             'use_noisy_q': True
diff --git a/src/nn/Linear.py b/src/nn/Linear.py
index bee37bd..a6842bb 100644
--- a/src/nn/Linear.py
+++ b/src/nn/Linear.py
@@ -1,7 +1,10 @@
+import math
+
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from torch.nn.parameter import Parameter
+import torch.nn.init as init
 
 from src.nn.activations import get_nn_activation
 
@@ -22,12 +25,14 @@ class NoisyLinear(nn.Linear):
 
     def reset_parameters(self):
         if hasattr(self, 'sigma_weight'):  # Only init after all params added (otherwise super().__init__() fails)
-            torch.nn.init.constant_(self.sigma_weight, self.sigma_init)
-            torch.nn.init.constant_(self.sigma_bias, self.sigma_init)
+            init.uniform_(self.weight, -math.sqrt(3 / self.in_features), math.sqrt(3 / self.in_features))
+            init.uniform_(self.bias, -math.sqrt(3 / self.in_features), math.sqrt(3 / self.in_features))
+            init.constant_(self.sigma_weight, self.sigma_init)
+            init.constant_(self.sigma_bias, self.sigma_init)
 
     def forward(self, x):
-        return F.linear(x, self.weight + self.sigma_weight * self.epsilon_weight,
-                        self.bias + self.sigma_bias * self.epsilon_bias)
+            return F.linear(x, self.weight + self.sigma_weight * self.epsilon_weight,
+                            self.bias + self.sigma_bias * self.epsilon_bias)
 
     def sample_noise(self):
         self.epsilon_weight = torch.randn(self.out_features, self.in_features)
@@ -56,7 +61,10 @@ class LinearModule(nn.Module):
             linear_layer = torch.nn.Linear(**linear_kwargs)
 
         self.linear_layer = linear_layer
-        self.dropout_layer = torch.nn.Dropout(dropout_p)
+        if dropout_p > 0.0:
+            self.dropout_layer = torch.nn.Dropout(dropout_p)
+        else:
+            self.dropout_layer = torch.nn.Identity()
         self.activation_layer = get_nn_activation(activation)
 
         self.weight_init = weight_init
@@ -92,6 +100,7 @@ class LinearModule(nn.Module):
                 pass
         elif weight_init == "xavier":
             torch.nn.init.xavier_uniform_(tensor.weight)
+            torch.nn.init.constant_(tensor.bias, 0.0)
         else:
             raise NotImplementedError("MLP initializer {} is not supported".format(weight_init))
 
diff --git a/src/nn/MLP.py b/src/nn/MLP.py
index ef4a5ab..f3a5058 100644
--- a/src/nn/MLP.py
+++ b/src/nn/MLP.py
@@ -12,7 +12,7 @@ class MLPConfig(ConfigBase):
             'input_dimension': 32,
             'output_dimension': 32,
             'activation': 'mish',
-            'out_activation': None,
+            'out_activation': 'mish',
             'num_neurons': [64, 64],
             'normalization': None,
             'weight_init': 'xavier',
diff --git a/src/nn/MultiStepInputGraphNetwork.py b/src/nn/MultiStepInputGraphNetwork.py
index 31ed335..4fcedf1 100644
--- a/src/nn/MultiStepInputGraphNetwork.py
+++ b/src/nn/MultiStepInputGraphNetwork.py
@@ -21,7 +21,7 @@ class MultiStepInputGraphNetworkConfig(ConfigBase):
 
         self.hist_rnn = {
             'rnn_type': 'GRU',
-            'input_size': 32,
+            'input_size': 16,
             'hidden_size': 64,
             'num_layers': 2,
             'batch_first': True}
diff --git a/src/nn/RelationalGraphNetwork.py b/src/nn/RelationalGraphNetwork.py
index 434e332..0117ffb 100644
--- a/src/nn/RelationalGraphNetwork.py
+++ b/src/nn/RelationalGraphNetwork.py
@@ -14,14 +14,14 @@ class RelationalGraphNetworkConfig(ConfigBase):
 
         self.gnn = {
             'input_node_dim': 19,
-            'hidden_node_dim': 8,
+            'hidden_node_dim': 16,
             'output_node_dim': 16,
             'init_node_dim': 19,
-            'num_hidden_layers': 2,
+            'num_hidden_layers': 1,
             'node_types': [NODE_ALLY, NODE_ENEMY],
             'edge_types': [EDGE_ALLY, EDGE_ENEMY, EDGE_ALLY_TO_ENEMY],
             'updater_conf': MLPConfig().mlp,
-            'use_residual': True,
+            'use_residual': False,
             'use_concat': True,
         }
 
diff --git a/src/rl/Qmixer.py b/src/rl/Qmixer.py
index 4281625..219cabd 100644
--- a/src/rl/Qmixer.py
+++ b/src/rl/Qmixer.py
@@ -9,7 +9,7 @@ from src.nn.RelationalGraphNetwork import RelationalGraphNetworkConfig as RGNCon
 from src.config.ConfigBase import ConfigBase
 
 from src.util.graph_util import get_filtered_node_index_by_type
-from src.config.graph_config import NODE_ALLY, EDGE_ALLY
+from src.config.graph_config import NODE_ALLY, NODE_ENEMY, EDGE_ALLY, EDGE_ALLY_TO_ENEMY, EDGE_ENEMY
 
 
 class QmixerConfig(ConfigBase):
@@ -26,6 +26,7 @@ class QmixerConfig(ConfigBase):
         self.w_net = RGNConfig().gnn
         self.w_net['input_node_dim'] = 51
         self.w_net['output_node_dim'] = self.mixer['num_clusters']
+        self.w_net['num_hidden_layers'] = 0
         self.w_net['node_types'] = [NODE_ALLY]
         self.w_net['edge_types'] = [EDGE_ALLY]
 
@@ -51,8 +52,11 @@ class Qmixer(nn.Module):
         ally_ws = torch.nn.functional.softmax(ally_ws, dim=1)
         return ally_ws
 
-    def get_feat(self, graph, node_feature):
-        ws = self.get_w(graph, node_feature)
+    def get_feat(self, graph, node_feature, ws=None):
+
+        if ws is None:
+            ws = self.get_w(graph, node_feature)
+
         ally_indices = get_filtered_node_index_by_type(graph, NODE_ALLY)
         ally_node_feature = node_feature[ally_indices, :]  # [#. allies x feature dim]
 
@@ -73,12 +77,14 @@ class Qmixer(nn.Module):
 
         return weighted_feat.transpose(2, 1)  # [#. graph x num_cluster x feature_dim]
 
-    def get_q(self, graph, node_feature, qs):
+    def get_q(self, graph, node_feature, qs, ws=None):
         device = node_feature.device
         ally_indices = get_filtered_node_index_by_type(graph, NODE_ALLY)
 
         # compute weighted sum of qs
-        ws = self.get_w(graph, node_feature)  # [#. allies x #. clusters]
+        if ws is None:
+            ws = self.get_w(graph, node_feature)  # [#. allies x #. clusters]
+
         weighted_q = qs.view(-1, 1) * ws  # [#. allies x #. clusters]
 
         qs = torch.zeros(size=(graph.number_of_nodes(), self.num_clusters), device=device)
@@ -100,7 +106,7 @@ class Qmixer(nn.Module):
 
     def forward(self, graph, node_feature, qs):
         ws = self.get_w(graph, node_feature)
-        aggregated_feat = self.get_feat(graph, node_feature)
+        aggregated_feat = self.get_feat(graph, node_feature, ws)
         aggregated_q = self.get_q(graph, node_feature, qs)
 
         ret_dict = dict()
diff --git a/src/test/test.py b/src/test/test.py
index e2bbd7e..4069bdc 100644
--- a/src/test/test.py
+++ b/src/test/test.py
@@ -6,7 +6,7 @@ from src.agent.QmixAgent import QmixAgent, QmixAgentConfig
 
 if __name__ == '__main__':
 
-    exp_name = "Qmix_MLPSUP"
+    exp_name = "qmix_refac"
 
     conf = QmixAgentConfig()
     use_noisy_q = conf.brain.brain['use_noisy_q']
@@ -38,7 +38,7 @@ if __name__ == '__main__':
         running_wr = np.mean(running_wrs)
 
         fit_return_dict.update({'train_winning_ratio': running_wr, 'epsilon': agent.brain.eps})
-
+        print(fit_return_dict)
         wandb.log(fit_return_dict, step=iters)
 
         if use_noisy_q:
